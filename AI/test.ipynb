{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea8a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from google import genai\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4ea9228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyDbBUXcGdZvcRT1KPDJc03Ozydbwb3Cfn4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MindPal_Pipeline:\n",
    "    def __init__(self):\n",
    "\n",
    "        GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        if not GOOGLE_API_KEY:\n",
    "            raise RuntimeError(\"Missing GOOGLE_API_KEY in environment variables.\")\n",
    "\n",
    "        self.model = \"gemini-2.5-pro\"\n",
    "\n",
    "        self.client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "        self.emotion_classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "            top_k=None,\n",
    "        )\n",
    "\n",
    "        self.SLANG_MAP = self.load_slang_dataset() or {}\n",
    "\n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        t = \" \".join(text.split()).strip()\n",
    "        t = t.lower()\n",
    "        t = t.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        t = contractions.fix(t)\n",
    "        return t\n",
    "\n",
    "    def load_slang_dataset(self) -> Dict[str, str]:\n",
    "        try:\n",
    "            ds = load_dataset(\"MLBtrio/genz-slang-dataset\", split=\"train\")\n",
    "            slang_map = {}\n",
    "            for row in ds:\n",
    "                key = row.get(\"Slang\")\n",
    "                desc = row.get(\"Description\")\n",
    "                if key:\n",
    "                    slang_map[key.lower()] = desc\n",
    "            return slang_map if slang_map else None\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to load GenZ slang dataset: {e}\")\n",
    "            return None\n",
    "\n",
    "    def detect_and_map_slang(self, text: str) -> str:\n",
    "        for s in self.SLANG_MAP if self.SLANG_MAP else {}:\n",
    "            if re.search(r\"\\b\" + re.escape(s) + r\"\\b\", text.lower()):\n",
    "                slang_token, meaning = s, self.SLANG_MAP[s]\n",
    "                replace = f\"{slang_token} ({meaning})\"\n",
    "                text = re.sub(\n",
    "                    r\"\\b\" + re.escape(slang_token) + r\"\\b\",\n",
    "                    replace,\n",
    "                    text,\n",
    "                    flags=re.IGNORECASE,\n",
    "                )\n",
    "        return text\n",
    "\n",
    "    def emotion_detection(self, text: str) -> str:\n",
    "        emotion = self.emotion_classifier(text)\n",
    "        return emotion[0]\n",
    "    \n",
    "    def emo_for_strategy(self, text: str) -> str:\n",
    "        emotion = self.emotion_detection(text)\n",
    "        top_emotion = emotion[0][\"label\"]\n",
    "        return top_emotion\n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        text: str,\n",
    "        detected_emotion: str,\n",
    "        history_messages: List[Tuple[str, str]] | None = None,\n",
    "        strategies: List | None = None\n",
    "    ) -> str:\n",
    "        # Build compact conversation context from history\n",
    "        history_context = \"\"\n",
    "        if history_messages:\n",
    "            # Router already limits and orders history; use as-is\n",
    "            recent = history_messages\n",
    "            lines = []\n",
    "            for role, msg in recent:\n",
    "                # guard against None and trim overly long single messages\n",
    "                safe_msg = (msg or \"\").strip()\n",
    "                if len(safe_msg) > 800:\n",
    "                    safe_msg = safe_msg[:800] + \" ...\"\n",
    "                lines.append(f\"{role}: {safe_msg}\")\n",
    "            history_context = \"\\n\".join(lines)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are MindPal, a supportive wellbeing chatbot for 13-15 year-old Australian teens.\n",
    "        Your responses should be:\n",
    "        - Warm, understanding, and age-appropriate\n",
    "        - Validate their feelings without being condescending\n",
    "        - Use language that feels natural to teens\n",
    "        - Acknowledge and reflect their feeling(s)\n",
    "        - Keep replies within 1-3 sentences and sound like a natural conversation\n",
    "        - Encourage them to talk more, ask follow up questions and let them express their feelings\n",
    "        - Encourage real-life support systems and resources\n",
    "        - When appropriate and you have enough information, gently encourage the teen to talk with a trusted adult or friend\n",
    "        - When appropriate, suggest the most suitable coping strategy from the list of coping strategies provided\n",
    "        - Avoid shaming or lecturing\n",
    "        - Use emojis to express emotions\n",
    "        - Do NOT encourage any dangerous behaviour or provide inappropriate information\n",
    "        - Do NOT give medical or clinical advice or replace professional help\n",
    "        - Do NOT be overly positive or negative, be neutral and honest when necessary\n",
    "        - Do NOT let the user give out any personal information\n",
    "        - If user asks questions that are unrelated to your purpose, politely decline to answer and redirect the conversation back\n",
    "\n",
    "        Current emotion(s) detected: {detected_emotion}\n",
    "        Conversation context: {history_context}\n",
    "        Child's current message: {text}\n",
    "        List of coping strategies based on the child's current emotion(s): {strategies}\n",
    "\n",
    "        Always rethink and double check your answer before responding.\n",
    "        When you completely understand you can start the session.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model, contents=prompt\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to generate response: {e}\")\n",
    "            return \"I'm sorry, I'm having trouble generating a response. Please try again later.\"\n",
    "\n",
    "        return response.text\n",
    "\n",
    "    def chat(\n",
    "        self, text: str, history_messages: List[Tuple[str, str]] | None = None\n",
    "    ) -> str:\n",
    "        text = self.normalize_text(text)\n",
    "        text = self.detect_and_map_slang(text)\n",
    "        detected_emotion = self.emotion_detection(text)\n",
    "        reply = self.generate_response(\n",
    "            text, detected_emotion, history_messages=history_messages\n",
    "        )\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "37540678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyra/code/MindPal_Backend/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness\n"
     ]
    }
   ],
   "source": [
    "emo = MindPal_Pipeline().emo_for_strategy(\"I'm sad\")\n",
    "print(emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b65af48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "analysis_tokenizer = AutoTokenizer.from_pretrained(\"Tianlin668/MentalBART\")\n",
    "analysis_model = AutoModelForSeq2SeqLM.from_pretrained(\"Tianlin668/MentalBART\")\n",
    "analysis_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f1b91012",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I don't think I have the energy to keep going anymore\"\n",
    "  \n",
    "prompt = (f\"\"\"\n",
    "    Analyse the following teen's chat message and provide a possible mental health condition and reasoning.\n",
    "    Teen's chat message: {text} \n",
    "\"\"\")\n",
    "\n",
    "inputs = analysis_tokenizer(prompt, return_tensors=\"pt\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "87b0ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suicide or self-harm tendency. \n",
      "The use of the phrase 'I don't think I have the energy to keep going anymore' suggests that the person is expressing feelings of hopelessness and a lack of motivation to continue living. This is a clear indication of suicidal ideation and a potential mental health issue.\n"
     ]
    }
   ],
   "source": [
    "# Generate continuation\n",
    "outputs = analysis_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.9,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Decode\n",
    "completion = analysis_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "condition = completion.split(\"Reasoning:\")[0]\n",
    "reasoning = completion.split(\"Reasoning: \")[1]\n",
    "print(condition) \n",
    "print(reasoning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73923ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Suicidality\n",
    "suicidality = pipeline(\"text-classification\", model=\"sentinet/suicidality\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7dcb1e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9977225661277771}]\n"
     ]
    }
   ],
   "source": [
    "result = suicidality(f\"{text}\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "18fc9c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Suicidal, Confidence: 0.46\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ethandavey/mental-health-diagnosis-bert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ethandavey/mental-health-diagnosis-bert\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare text\n",
    "# text = \"I bought a rope and I'm thinking about using it to hang myself.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = F.softmax(outputs.logits, dim=1)\n",
    "\n",
    "# Map prediction to label\n",
    "label_mapping = {0: \"Anxiety\", 1: \"Normal\", 2: \"Depression\", 3: \"Suicidal\", 4: \"Stress\"}\n",
    "predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "prediction = label_mapping[predicted_class]\n",
    "confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "print(f\"Prediction: {prediction}, Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1affe5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
